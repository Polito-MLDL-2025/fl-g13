{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed3f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr\n",
    "import torch\n",
    "import dotenv\n",
    "import wandb\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from fl_g13.architectures import BaseDino\n",
    "from fl_g13.editing import SparseSGDM\n",
    "\n",
    "from fl_g13.fl_pytorch import build_fl_dependencies\n",
    "\n",
    "from fl_g13.fl_pytorch.datasets import reset_partition\n",
    "from fl_g13.modeling import load_or_create\n",
    "from fl_g13.editing import SparseSGDM\n",
    "from torch.optim import SGD\n",
    "\n",
    "from fl_g13.fl_pytorch import get_client_app, get_server_app\n",
    "from flwr.simulation import run_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "build_fl_dependencies()\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0}}\n",
    "else:\n",
    "    backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4be89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login by key in .env file\n",
    "WANDB_API_KEY = dotenv.dotenv_values()[\"WANDB_API_KEY\"]\n",
    "wandb.login(key=WANDB_API_KEY)\n",
    "\n",
    "# Load checkpoint from .env file\n",
    "CHECKPOINT_DIR = dotenv.dotenv_values()[\"CHECKPOINT_DIR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9668493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "## Model Hyper-parameters\n",
    "head_layers = 3\n",
    "head_hidden_size = 512\n",
    "dropout_rate = 0.0\n",
    "unfreeze_blocks = 12\n",
    "\n",
    "## Training Hyper-parameters\n",
    "batch_size = 128\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-5\n",
    "T_max = 8\n",
    "eta_min = 1e-5\n",
    "\n",
    "# FL config\n",
    "K = 100\n",
    "C = 0.1\n",
    "J = 8\n",
    "num_shards_per_partition = 10 # Nc\n",
    "partition_type = 'shard'\n",
    "\n",
    "num_rounds = 100\n",
    "\n",
    "## Server App config\n",
    "save_every = 5\n",
    "evaluate_each = 5\n",
    "fraction_fit = C        # 0.1\n",
    "fraction_evaluate = C   # 0.1\n",
    "min_fit_clients = 10\n",
    "min_evaluate_clients = 5\n",
    "min_available_clients = 10\n",
    "\n",
    "# model editing config\n",
    "model_editing = True\n",
    "mask_type = 'local'\n",
    "sparsity = 0.7\n",
    "calibration_rounds = 3\n",
    "model_editing_batch_size = 1\n",
    "mask = None\n",
    "\n",
    "## simulation run config\n",
    "NUM_CLIENTS = 100\n",
    "MAX_PARALLEL_CLIENTS = 10\n",
    "\n",
    "## Base model location\n",
    "# The 200-epoch model folder\n",
    "# Ensure that the most recent file is the correct one\n",
    "model_save_path = CHECKPOINT_DIR + f\"/fl/non-iid/{num_shards_per_partition}_{J}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d9cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulations\n",
    "reset_partition()\n",
    "\n",
    "# print('-' * 200)\n",
    "# print(f\" Nc={num_shards_per_partition}, J={J}, mask_type={mask_type}, sparsity={sparsity}, mask_calibration_round={calibration_rounds}\\n\")\n",
    "\n",
    "model_checkpoint = CHECKPOINT_DIR + f\"/fl/non-iid/Baseline/{num_shards_per_partition}_{J}_{mask_type}_{sparsity}_{calibration_rounds}\"\n",
    "\n",
    "# Load Base model\n",
    "model, start_epoch = load_or_create(\n",
    "    path=model_save_path,\n",
    "    model_class=BaseDino,\n",
    "    model_config=None,\n",
    "    optimizer=None,\n",
    "    scheduler=None,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "if model_editing:\n",
    "    # Create a dummy mask for SparseSGDM\n",
    "    dummy_mask = [torch.ones_like(p, device=p.device) for p in model.parameters()]  \n",
    "    \n",
    "    optimizer = SparseSGDM(\n",
    "        model.parameters(),\n",
    "        mask=dummy_mask,\n",
    "        lr=lr,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "else:\n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer=optimizer,\n",
    "    T_max=T_max,\n",
    "    eta_min=eta_min\n",
    ")\n",
    "\n",
    "# Unfreeze entire model for model_editing\n",
    "model.unfreeze_blocks(unfreeze_blocks)\n",
    "\n",
    "# Wandb settings\n",
    "use_wandb = True\n",
    "run_name = f\"fl_bl_{num_shards_per_partition}_{J}_{mask_type}_{sparsity}_{calibration_rounds}\"\n",
    "wandb_config = {\n",
    "    # wandb param\n",
    "    'name': run_name,\n",
    "    'project_name': f\"fl_v5_{num_shards_per_partition}_{J}_baseline\",\n",
    "    'run_id': run_name,\n",
    "    \n",
    "    # fl config\n",
    "    \"fraction_fit\": fraction_fit,\n",
    "    \"lr\": lr,\n",
    "    \"momentum\": momentum,\n",
    "    'weight_decay': weight_decay,\n",
    "    'partition_type': partition_type,\n",
    "    'K': K,\n",
    "    'C': C,\n",
    "    'J': J,\n",
    "    'Nc': num_shards_per_partition,\n",
    "    \n",
    "    # model config\n",
    "    'head_layers': head_layers,\n",
    "    'head_hidden_size': head_hidden_size,\n",
    "    'dropout_rate': dropout_rate,\n",
    "    'unfreeze_blocks': unfreeze_blocks,\n",
    "    \n",
    "    # model editing config\n",
    "    'model_editing_batch_size': model_editing_batch_size,\n",
    "    'mask_calibration_round': calibration_rounds,\n",
    "    'mask_type': mask_type,\n",
    "    'sparsity': sparsity\n",
    "}\n",
    "\n",
    "client = get_client_app(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=DEVICE,\n",
    "    partition_type=partition_type,\n",
    "    local_epochs=1,\n",
    "    local_steps=J,\n",
    "    batch_size=batch_size,\n",
    "    num_shards_per_partition=num_shards_per_partition,\n",
    "    scheduler=scheduler,\n",
    "    model_editing=model_editing,\n",
    "    mask_type=mask_type,\n",
    "    sparsity=sparsity,\n",
    "    mask=mask,\n",
    "    model_editing_batch_size=model_editing_batch_size,\n",
    "    mask_func=None,\n",
    "    mask_calibration_round=calibration_rounds\n",
    ")\n",
    "\n",
    "server = get_server_app(\n",
    "    checkpoint_dir=model_checkpoint,\n",
    "    model_class=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    num_rounds=num_rounds,\n",
    "    fraction_fit=fraction_fit,\n",
    "    fraction_evaluate=fraction_evaluate,\n",
    "    min_fit_clients=min_fit_clients,\n",
    "    min_evaluate_clients=min_evaluate_clients,\n",
    "    min_available_clients=min_available_clients,\n",
    "    device=DEVICE,\n",
    "    use_wandb=use_wandb,\n",
    "    wandb_config=wandb_config,\n",
    "    save_every=save_every,\n",
    "    prefix='baseline',\n",
    "    evaluate_each=evaluate_each,\n",
    "    model= model,\n",
    "    start_epoch= start_epoch\n",
    ")\n",
    "\n",
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config\n",
    ")\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
