{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbee39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f52dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-25 10:51:59.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfl_g13.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /home/massimiliano/Projects/fl-g13\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, RandomCrop, RandomHorizontalFlip, RandomVerticalFlip, Normalize, ToTensor\n",
    "\n",
    "from fl_g13.config import RAW_DATA_DIR\n",
    "from fl_g13.modeling import train, eval, save, load, backup, load_loss_and_accuracies, save_loss_and_accuracy\n",
    "from fl_g13.dataset import train_test_split\n",
    "\n",
    "from fl_g13.architectures import BaseDino\n",
    "from fl_g13.editing import SparseSGDM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f11d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 40000\n",
      "Validation dataset size: 10000\n",
      "Test dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Define preprocessing pipeline\n",
    "train_transform = Compose([\n",
    "    Resize(256), # CIFRA100 is originally 32x32\n",
    "    RandomCrop(224), # But Dino works on 224x224\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    #Normalize(mean=[0.5071, 0.4866, 0.4409], std=[0.2673, 0.2564, 0.2762]), # CIFRAR100 Train stats\n",
    "    Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) # Use ImageNet stats\n",
    "])\n",
    "\n",
    "eval_transform = Compose([\n",
    "    Resize(256), # CIFRA100 is originally 32x32\n",
    "    CenterCrop(224), # But Dino works on 224x224\n",
    "    ToTensor(),\n",
    "    #Normalize(mean=[0.5071, 0.4866, 0.4409], std=[0.2673, 0.2564, 0.2762]), # CIFRAR100 Train stats\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Use ImageNet stats\n",
    "])\n",
    "\n",
    "cifar100_train = datasets.CIFAR100(root=RAW_DATA_DIR, train=True, download=True, transform=train_transform)\n",
    "cifar100_test = datasets.CIFAR100(root=RAW_DATA_DIR, train=False, download=True, transform=eval_transform)\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(cifar100_train, 0.8, random_state=None)\n",
    "test_dataset = cifar100_test\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a5230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/massimiliano/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: BaseDino(\n",
      "  (net): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=512, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (4): GELU(approximate='none')\n",
      "      (5): Dropout(p=0.0, inplace=False)\n",
      "      (6): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (7): GELU(approximate='none')\n",
      "      (8): Dropout(p=0.0, inplace=False)\n",
      "      (9): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (10): GELU(approximate='none')\n",
      "      (11): Dropout(p=0.0, inplace=False)\n",
      "      (12): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (13): GELU(approximate='none')\n",
      "      (14): Dropout(p=0.0, inplace=False)\n",
      "      (15): Linear(in_features=512, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Move to CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Settings\n",
    "CHECKPOINT_DIR = \"/home/massimiliano/Projects/fl-g13/checkpoints\"\n",
    "name = \"uxie\"\n",
    "start_epoch=1\n",
    "num_epochs=60\n",
    "save_every=5\n",
    "backup_every=10\n",
    "\n",
    "# Hyper-parameters\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "\n",
    "# Dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model\n",
    "model = BaseDino(head_layers=5, head_hidden_size=512, dropout_rate=0.0, unfreeze_blocks=1)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer, scheduler, and loss function\n",
    "mask = [torch.ones_like(p, device=p.device) for p in model.parameters()] # Must be done AFTER the model is moved to CUDA\n",
    "optimizer = SparseSGDM(model.parameters(), mask=mask, lr=LR)\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer, \n",
    "    T_0=15,          # First restart after 12 epochs\n",
    "    T_mult=2,        # Double the interval between restarts each time\n",
    "    eta_min=1e-7     # Minimum learning rate after annealing\n",
    ")\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "all_training_losses=[]       # Pre-allocated list for training losses\n",
    "all_validation_losses=[]       # Pre-allocated list for validation losses\n",
    "all_training_accuracies=[]    # Pre-allocated list for training accuracies\n",
    "all_validation_accuracies=[]    # Pre-allocated list for validation accuracies\n",
    "\n",
    "# # Model loading (uncomment to properly overwrite)\n",
    "# loading_epoch = 50\n",
    "# model, start_epoch = load(\n",
    "#     f\"{CHECKPOINT_DIR}/BaseDino/{name}_BaseDino_epoch_{loading_epoch}.pth\",\n",
    "#     model_class=BaseDino,\n",
    "#     device=device,\n",
    "#     optimizer=optimizer,\n",
    "#     scheduler=scheduler,\n",
    "#     verbose=True\n",
    "# )\n",
    "# model.to(device)\n",
    "# loaded_metrics = load_loss_and_accuracies(path=f\"{CHECKPOINT_DIR}/BaseDino/{name}_BaseDino_epoch_{loading_epoch}.loss_acc.json\")\n",
    "\n",
    "# # Preallocated lists: if the training interrupts, it will still save their values (uncomment to properly load and overwrite)\n",
    "# all_training_losses=loaded_metrics[\"train_loss\"]       # Pre-allocated list for training losses\n",
    "# all_validation_losses=loaded_metrics[\"val_loss\"]       # Pre-allocated list for validation losses\n",
    "# all_training_accuracies=loaded_metrics[\"train_acc\"]    # Pre-allocated list for training accuracies\n",
    "# all_validation_accuracies=loaded_metrics[\"val_acc\"]    # Pre-allocated list for validation accuracies\n",
    "\n",
    "print(f\"\\nModel: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feed087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix/name for the model was provided: uxie\n",
      "\n",
      "üöÄ Epoch 1/60 (1.67%) Completed\n",
      "\tüìä Training Loss: 2.5347\n",
      "\t‚úÖ Training Accuracy: 43.90%\n",
      "\t‚è≥ Elapsed Time: 84.69s | ETA: 4996.44s\n",
      "\tüïí Completed At: 10:53\n",
      "üîç Validation Results:\n",
      "\tüìâ Validation Loss: 1.5047\n",
      "\tüéØ Validation Accuracy: 58.70%\n",
      "\n",
      "üöÄ Epoch 2/60 (3.33%) Completed\n",
      "\tüìä Training Loss: 1.1572\n",
      "\t‚úÖ Training Accuracy: 66.49%\n",
      "\t‚è≥ Elapsed Time: 85.11s | ETA: 4936.19s\n",
      "\tüïí Completed At: 10:55\n",
      "üîç Validation Results:\n",
      "\tüìâ Validation Loss: 1.3548\n",
      "\tüéØ Validation Accuracy: 61.81%\n",
      "\n",
      "üöÄ Epoch 3/60 (5.00%) Completed\n",
      "\tüìä Training Loss: 0.9762\n",
      "\t‚úÖ Training Accuracy: 71.04%\n",
      "\t‚è≥ Elapsed Time: 84.81s | ETA: 4834.07s\n",
      "\tüïí Completed At: 10:56\n",
      "üîç Validation Results:\n",
      "\tüìâ Validation Loss: 1.1677\n",
      "\tüéØ Validation Accuracy: 66.43%\n",
      "\n",
      "üöÄ Epoch 4/60 (6.67%) Completed\n",
      "\tüìä Training Loss: 0.8724\n",
      "\t‚úÖ Training Accuracy: 73.90%\n",
      "\t‚è≥ Elapsed Time: 84.69s | ETA: 4742.43s\n",
      "\tüïí Completed At: 10:58\n",
      "üîç Validation Results:\n",
      "\tüìâ Validation Loss: 1.1391\n",
      "\tüéØ Validation Accuracy: 67.06%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _, _, _, _ = train(\n",
    "        checkpoint_dir=CHECKPOINT_DIR,\n",
    "        name=name,\n",
    "        start_epoch=start_epoch,\n",
    "        num_epochs=num_epochs,\n",
    "        save_every=save_every,\n",
    "        backup_every=backup_every,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        verbose=False,\n",
    "        all_training_losses=all_training_losses,\n",
    "        all_validation_losses=all_validation_losses,\n",
    "        all_training_accuracies=all_training_accuracies,\n",
    "        all_validation_accuracies=all_validation_accuracies,\n",
    "    )\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted manually.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Training stopped due to error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(all_training_losses, label='Training Loss')\n",
    "plt.plot(all_validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(all_training_accuracies, label='Training Accuracy')\n",
    "plt.plot(all_validation_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation dataset\n",
    "test_loss, test_accuracy, _ = eval(dataloader=test_dataloader, model=model, criterion=criterion)\n",
    "\n",
    "print(\n",
    "    f\"üîç Test Results:\\n\"\n",
    "    f\"\\tüìâ Test Loss: {test_loss:.4f}\\n\"\n",
    "    f\"\\tüéØ Test Accuracy: {100 * test_accuracy:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6b2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-g13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
