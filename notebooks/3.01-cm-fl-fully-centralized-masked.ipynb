{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d326f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f626291",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:24:24.400308Z",
     "start_time": "2025-05-03T07:24:14.251033Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 21:15:48.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfl_g13.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /home/massimiliano/Projects/fl-g13\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flower 1.17.0 / PyTorch 2.6.0+cu124\n",
      "'vision_transformer.py' already exists.\n",
      "'utils.py' already exists.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fl_g13.editing.sparseSGDM import SparseSGDM\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import flwr\n",
    "from flwr.simulation import run_simulation\n",
    "from fl_g13.architectures import BaseDino\n",
    "from fl_g13.fl_pytorch import get_client_app, get_server_app\n",
    "from fl_g13.fl_pytorch import build_fl_dependencies\n",
    "from fl_g13.fl_pytorch import FullyCentralizedMaskedFedAvg, CustomFedAvg\n",
    "\n",
    "\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "\n",
    "build_fl_dependencies() #! Remind to always put this, it will download Dino dependencies for client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e9d3ca8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:24:24.415306Z",
     "start_time": "2025-05-03T07:24:24.401288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint saving settings\n",
    "CHECKPOINT_DIR = \"/home/massimiliano/Projects/fl-g13/checkpoints\"\n",
    "name = 'aron'\n",
    "save_with_model_dir = False\n",
    "save_every = 1\n",
    "\n",
    "# Model hyper-parameters\n",
    "head_layers=3\n",
    "head_hidden_size=512\n",
    "dropout_rate=0.0\n",
    "unfreeze_blocks=1\n",
    "\n",
    "# Training hyper-parameters\n",
    "starting_lr = 1e-3\n",
    "momentum = 0.9\n",
    "weight_decay=1e-5\n",
    "T_max=8\n",
    "eta_min=1e-5\n",
    "\n",
    "# Federated Training settings\n",
    "batch_size = 64 # Batch size for training #! Let's stick to 64 to make training fit also on RTX 3070\n",
    "local_epochs = 1 # Number of local epochs per client\n",
    "number_of_rounds = 5 # Total number of federated learning rounds\n",
    "fraction_fit = 1 # Fraction of clients participating in training per round\n",
    "fraction_evaluate = 0.1 # Fraction of clients participating in evaluation per round\n",
    "number_of_clients = 2 # Total number of clients in the simulation\n",
    "min_num_clients = 2 # Minimum number of clients required for training and evaluation\n",
    "partition_type = \"iid\" # Partitioning strategy for the dataset (e.g., \"iid\" or \"shard\")\n",
    "num_shards_per_partition = 6 # Number of shards per partition (used when partition_type is \"shard\")\n",
    "use_wandb = False # Whether to use Weights & Biases (wandb) for experiment tracking\n",
    "wandb_config = None\n",
    "\n",
    "# Device settings\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "backend_config = {\n",
    "    \"client_resources\": {\n",
    "        \"num_cpus\": 1, \n",
    "        \"num_gpus\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# When running on GPU, assign an entire GPU for each client\n",
    "# Refer to Flower framework documentation for more details about Flower simulations\n",
    "# and how to set up the `backend_config`\n",
    "if device == \"cuda\":\n",
    "    backend_config[\"client_resources\"] = {\n",
    "        \"num_cpus\": 1, \n",
    "        \"num_gpus\": 1\n",
    "    }\n",
    "\n",
    "print(f\"Training on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aebfcdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:24:34.527814Z",
     "start_time": "2025-05-03T07:24:32.944367Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/massimiliano/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No checkpoint found at /home/massimiliano/Projects/fl-g13/checkpoints. Creating a new model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/massimiliano/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = BaseDino(\n",
    "    head_layers=head_layers, \n",
    "    head_hidden_size=head_hidden_size, \n",
    "    dropout_rate=dropout_rate, \n",
    "    unfreeze_blocks=unfreeze_blocks\n",
    "    )\n",
    "model.to(device)\n",
    "\n",
    "mask = [torch.ones_like(p, device=p.device) for p in model.parameters()] # Must be done AFTER the model is moved to CUDA\n",
    "optimizer = SparseSGDM(\n",
    "    params=model.parameters(),\n",
    "    mask=mask,\n",
    "    lr=starting_lr,\n",
    "    momentum=momentum,\n",
    "    weight_decay=weight_decay\n",
    "    )\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer=optimizer, \n",
    "    T_max=T_max, \n",
    "    eta_min=eta_min\n",
    "    )\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "client_app = get_client_app(\n",
    "    model=model, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    strategy='fully_centralized',\n",
    "    partition_type=partition_type, \n",
    "    batch_size=batch_size,\n",
    "    num_shards_per_partition=num_shards_per_partition,\n",
    "    local_epochs=local_epochs,\n",
    "    model_editing=False,\n",
    "    mask_type= 'global',\n",
    "    sparsity = 0.2,\n",
    ")\n",
    "\n",
    "server_app = get_server_app(\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    prefix=name,\n",
    "    model_class=model.__class__,\n",
    "    model_config=model.get_config(), \n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    device=device, \n",
    "    save_every=save_every,\n",
    "    save_with_model_dir=save_with_model_dir,\n",
    "    strategy='fully_centralized',\n",
    "    num_rounds=number_of_rounds, \n",
    "    fraction_fit=fraction_fit,\n",
    "    fraction_evaluate=fraction_evaluate,\n",
    "    min_fit_clients=min_num_clients,\n",
    "    min_evaluate_clients=min_num_clients,\n",
    "    min_available_clients=number_of_clients,\n",
    "    use_wandb=use_wandb,\n",
    "    wandb_config=wandb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9cac28",
   "metadata": {},
   "source": [
    "### Pre-train the model (head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ecad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Server on device: cuda:0\n",
      "[Server] CUDA available in client: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=5, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using strategy 'CentralizedMaskedFedAvg'\n",
      "[Server Eval Round 0] Model device: cuda:0\n",
      "[Server Eval Round 0] CUDA available in server eval: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval progress: 100%|██████████| 313/313 [00:24<00:00, 12.56batch/s]\n",
      "\u001b[92mINFO \u001b[0m:      [Round 0] Centralized Evaluation - Loss: 6.4040, Metrics: {'centralized_accuracy': 0.0114}\n",
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 6.404029466854498, {'centralized_accuracy': 0.0114}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered server configure_fit()\n",
      "Number of client instructions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 2 failures\n",
      "\u001b[92mINFO \u001b[0m:      [Round 1] No aggregated parameters (possibly all clients failed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered server aggregate_fit()\n",
      "Warmup complete after 1 rounds, switching to mask calibration\n",
      "[Server Eval Round 1] Model device: cuda:0\n",
      "[Server Eval Round 1] CUDA available in server eval: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval progress: 100%|██████████| 313/313 [00:24<00:00, 12.81batch/s]\n",
      "\u001b[92mINFO \u001b[0m:      [Round 1] Centralized Evaluation - Loss: 6.4040, Metrics: {'centralized_accuracy': 0.0114}\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (1, 6.404029466854498, {'centralized_accuracy': 0.0114}, 25.096792156000447)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n",
      "\u001b[36m(ClientAppActor pid=149046)\u001b[0m 2025-05-14 21:16:43.068 | INFO     | fl_g13.config:<module>:11 - PROJ_ROOT path is: /home/massimiliano/Projects/fl-g13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=149046)\u001b[0m [Client] Client on device: cuda:0\n",
      "\u001b[36m(ClientAppActor pid=149046)\u001b[0m [Client] CUDA available in client: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval progress:   0%|          | 0/79 [00:00<?, ?batch/s]\n",
      "Eval progress:   1%|▏         | 1/79 [00:00<00:22,  3.44batch/s]\n",
      "Eval progress:   3%|▎         | 2/79 [00:00<00:17,  4.50batch/s]\n",
      "Eval progress:   4%|▍         | 3/79 [00:00<00:15,  5.02batch/s]\n",
      "Eval progress:   5%|▌         | 4/79 [00:00<00:14,  5.30batch/s]\n",
      "Eval progress:   6%|▋         | 5/79 [00:00<00:13,  5.48batch/s]\n",
      "Eval progress:   8%|▊         | 6/79 [00:01<00:13,  5.57batch/s]\n",
      "Eval progress:   9%|▉         | 7/79 [00:01<00:12,  5.66batch/s]\n",
      "Eval progress:  10%|█         | 8/79 [00:01<00:12,  5.71batch/s]\n",
      "Eval progress:  11%|█▏        | 9/79 [00:01<00:12,  5.73batch/s]\n",
      "Eval progress:  13%|█▎        | 10/79 [00:01<00:12,  5.75batch/s]\n",
      "Eval progress:  14%|█▍        | 11/79 [00:02<00:11,  5.76batch/s]\n",
      "Eval progress:  15%|█▌        | 12/79 [00:02<00:11,  5.75batch/s]\n",
      "Eval progress:  16%|█▋        | 13/79 [00:02<00:11,  5.78batch/s]\n"
     ]
    }
   ],
   "source": [
    "run_simulation(\n",
    "    client_app=client_app,\n",
    "    server_app=server_app,\n",
    "    num_supernodes=number_of_clients,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d49c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00169f65",
   "metadata": {},
   "source": [
    "Otherwise... We could do:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da863b4e",
   "metadata": {},
   "source": [
    "### Compute Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e111fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_simulation(\n",
    "#     client_app=client_app,\n",
    "#     server_app=server_app,\n",
    "#     num_supernodes=number_of_clients,\n",
    "#     backend_config=backend_config,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657801f",
   "metadata": {},
   "source": [
    "### Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e94d4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-03T07:24:34.559663Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# run_simulation(\n",
    "#     client_app=client_app,\n",
    "#     server_app=server_app,\n",
    "#     num_supernodes=number_of_clients,\n",
    "#     backend_config=backend_config,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c258a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-g13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
