{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6141bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c40964",
   "metadata": {},
   "source": [
    "# Import cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import dotenv\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from fl_g13.config import RAW_DATA_DIR\n",
    "\n",
    "from fl_g13.modeling import train, load, eval, get_preprocessing_pipeline, plot_metrics\n",
    "\n",
    "from fl_g13.architectures import BaseDino\n",
    "\n",
    "from fl_g13.editing import SparseSGDM\n",
    "from fl_g13.editing import create_mask, mask_dict_to_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fbb650",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44323213",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "CHECKPOINT_DIR = dotenv.dotenv_values()[\"CHECKPOINT_DIR\"]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64cdfaf",
   "metadata": {},
   "source": [
    "# Validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d43e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = get_preprocessing_pipeline(RAW_DATA_DIR)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13feaa1",
   "metadata": {},
   "source": [
    "# Full training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6abe713",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset, full_test_dataset = get_preprocessing_pipeline(RAW_DATA_DIR, do_full_training = True)\n",
    "\n",
    "print(f\"Full Train dataset size: {len(full_train_dataset)}\")\n",
    "print(f\"Full Test dataset size: {len(full_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3e31c2",
   "metadata": {},
   "source": [
    "# Define model to edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba9d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "name = \"arcanine\"\n",
    "\n",
    "# Model Hyper-parameters\n",
    "head_layers = 3\n",
    "head_hidden_size = 512\n",
    "dropout_rate = 0.0\n",
    "unfreeze_blocks = 0\n",
    "\n",
    "# Training Hyper-parameters\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-5\n",
    "T_max = 8\n",
    "eta_min = 1e-5\n",
    "\n",
    "# Base Model\n",
    "model = BaseDino(\n",
    "    head_layers=head_layers, \n",
    "    head_hidden_size=head_hidden_size, \n",
    "    dropout_rate=dropout_rate, \n",
    "    unfreeze_blocks=unfreeze_blocks\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Optimizer, scheduler, and loss function\n",
    "dummy_mask = [torch.ones_like(p, device=p.device) for p in model.parameters()]\n",
    "optimizer = SparseSGDM(\n",
    "    model.parameters(),\n",
    "    mask=dummy_mask,\n",
    "    lr=lr,\n",
    "    momentum=momentum,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer=optimizer, \n",
    "    T_max=T_max, \n",
    "    eta_min=eta_min\n",
    ")\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Load arcanine\n",
    "loading_epoch = 10\n",
    "loading_model_path =  f\"{CHECKPOINT_DIR}/{name}/{name}_BaseDino_epoch_{loading_epoch}.pth\"\n",
    "model, start_epoch = load(\n",
    "    loading_model_path,\n",
    "    model_class=BaseDino,\n",
    "    device=DEVICE,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    verbose=True\n",
    ")\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fdc702",
   "metadata": {},
   "source": [
    "# Create mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad0ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centralized_model_mask(model, dataloader, sparsity, mask_type, calibration_rounds, file_path = 'centralized_model_mask.pth', verbose = False):\n",
    "    if file_path and os.path.isfile(file_path):\n",
    "        if verbose:\n",
    "            print(f'[CMM] Found {file_path}. Loading mask from memory')\n",
    "            \n",
    "        return torch.load(file_path)\n",
    "    \n",
    "    # else    \n",
    "    if verbose:\n",
    "        print('[CMM] Computing mask')\n",
    "    mask = create_mask(\n",
    "        dataloader, \n",
    "        model, \n",
    "        sparsity = sparsity, \n",
    "        mask_type = mask_type, \n",
    "        rounds = calibration_rounds, \n",
    "        verbose = verbose\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'[CMM] Saving the mask at \"{file_path}\"')\n",
    "    torch.save(mask, file_path)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f16e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = .9\n",
    "mask_type = 'global'\n",
    "calibration_rounds = 3\n",
    "unfreeze_blocks = 12\n",
    "fisher_dataloader = DataLoader(full_train_dataset, batch_size = 1, shuffle=True)\n",
    "\n",
    "me_model_name = f'{name}_{loading_epoch}_{mask_type}_{sparsity}_{calibration_rounds}'\n",
    "file_path = CHECKPOINT_DIR + f'/masks/{me_model_name}.pth'\n",
    "\n",
    "# Unfreeze the model before computing the mask\n",
    "model.unfreeze_blocks(unfreeze_blocks)\n",
    "mask = get_centralized_model_mask(model, fisher_dataloader, sparsity, mask_type, calibration_rounds, file_path, verbose = True)\n",
    "mask_list = mask_dict_to_list(model, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f8de3a",
   "metadata": {},
   "source": [
    "# Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ab941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(\n",
    "    starting_model_path, \n",
    "    model_name, \n",
    "    train_dataloader, \n",
    "    test_dataloader, \n",
    "    val_dataloader, \n",
    "    mask, \n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    criterion, \n",
    "    epochs = 10, \n",
    "    verbose = 1\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the model\n",
    "    new_model, start_epoch = load(\n",
    "        path = starting_model_path,\n",
    "        model_class = BaseDino,\n",
    "        optimizer = optimizer,\n",
    "        scheduler = scheduler,\n",
    "        device = device\n",
    "    )\n",
    "    new_model.to(device) # manually move the model to the device\n",
    "\n",
    "    # unfreeze the model\n",
    "    unfreeze_blocks = 12\n",
    "    new_model.unfreeze_blocks(unfreeze_blocks)\n",
    "\n",
    "    # Create a new SparseSGDM optimizer\n",
    "    new_optimizer = SparseSGDM(\n",
    "        new_model.parameters(), \n",
    "        mask = mask, \n",
    "        lr = lr,\n",
    "        momentum = momentum,\n",
    "        weight_decay = weight_decay\n",
    "    )\n",
    "\n",
    "    try: \n",
    "        _, _, _, _ = train(\n",
    "            checkpoint_dir = f'{CHECKPOINT_DIR}/{model_name}',\n",
    "            name = model_name,\n",
    "            start_epoch = start_epoch,\n",
    "            num_epochs = epochs,\n",
    "            save_every = 1,\n",
    "            backup_every = None,\n",
    "            train_dataloader = train_dataloader,\n",
    "            val_dataloader = val_dataloader,\n",
    "            model = new_model,\n",
    "            criterion = criterion,\n",
    "            optimizer = new_optimizer,\n",
    "            scheduler = scheduler,\n",
    "            verbose = verbose,\n",
    "            with_model_dir = False\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted manually.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training stopped due to error: {e}\")\n",
    "\n",
    "    # Final eval\n",
    "    if test_dataloader:\n",
    "        test_loss, test_accuracy, _ = eval(dataloader=test_dataloader, model=new_model, criterion=criterion)\n",
    "        return test_loss, test_accuracy\n",
    "    else:\n",
    "        return -1, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5155eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Validation dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Full Training dataloaders\n",
    "full_train = DataLoader(full_train_dataset, batch_size = batch_size, shuffle = True)\n",
    "full_test = DataLoader(full_test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b5d4d",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9e8c53",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eb1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_epoch = 30\n",
    "val_model_name = 'val_arcanine'\n",
    "starting_model_path = f\"{CHECKPOINT_DIR}/{val_model_name}/{val_model_name}_BaseDino_epoch_{loading_epoch}.pth\"\n",
    "\n",
    "# Validation\n",
    "_, _ = fine_tune(\n",
    "    starting_model_path = starting_model_path,\n",
    "    model_name = f'me_{me_model_name}',\n",
    "    train_dataloader = train_dataloader,\n",
    "    test_dataloader = None,\n",
    "    val_dataloader = val_dataloader,\n",
    "    mask = mask_list,\n",
    "    optimizer = optimizer,\n",
    "    scheduler = scheduler,\n",
    "    criterion = criterion,\n",
    "    epochs = val_epoch - loading_epoch, # to get to 30\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "# plot metrics\n",
    "metrics_data = f\"{CHECKPOINT_DIR}/{f'me_{me_model_name}'}/{f'me_{me_model_name}'}_BaseDino_epoch_{val_epoch}.loss_acc.json\"\n",
    "plot_metrics(path = metrics_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7204498",
   "metadata": {},
   "source": [
    "# Full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "base_model = 'arcanine'\n",
    "starting_model_path = f\"{CHECKPOINT_DIR}/{base_model}/{base_model}_BaseDino_epoch_{loading_epoch}.pth\"\n",
    "me_model_name = 'arcanine_talos'\n",
    "me_test_loss, me_test_acc = fine_tune(\n",
    "    starting_model_path = loading_model_path,\n",
    "    model_name = me_model_name,\n",
    "    train_dataloader = full_train,\n",
    "    test_dataloader = full_test,\n",
    "    val_dataloader = None,\n",
    "    mask = mask_list,\n",
    "    optimizer = optimizer,\n",
    "    scheduler = scheduler,\n",
    "    criterion = criterion,\n",
    "    epochs = num_epochs - loading_epoch, # to get to 30\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"🔍 Test Results:\\n\"\n",
    "    f\"\\t📉 Test Loss: {me_test_loss:.4f}\\n\"\n",
    "    f\"\\t🎯 Test Accuracy: {100 * me_test_acc:.2f}%\"\n",
    ")\n",
    "\n",
    "# Plot metrics\n",
    "metrics_data = f'{CHECKPOINT_DIR}/{me_model_name}/{me_model_name}_BaseDino_epoch_{num_epochs}.loss_acc.json'\n",
    "plot_metrics(path = metrics_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
