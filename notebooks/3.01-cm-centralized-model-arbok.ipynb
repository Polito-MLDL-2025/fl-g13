{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbee39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f52dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, RandomCrop, RandomHorizontalFlip, RandomVerticalFlip, Normalize, ToTensor\n",
    "\n",
    "from fl_g13.config import RAW_DATA_DIR\n",
    "from fl_g13.modeling import train, eval, save, load, backup, load_loss_and_accuracies, save_loss_and_accuracy\n",
    "from fl_g13.dataset import train_test_split\n",
    "\n",
    "from fl_g13.architectures import BaseDino\n",
    "from fl_g13.editing import SparseSGDM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f11d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing pipeline\n",
    "train_transform = Compose([\n",
    "    Resize(256), # CIFRA100 is originally 32x32\n",
    "    RandomCrop(224), # But Dino works on 224x224\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    #Normalize(mean=[0.5071, 0.4866, 0.4409], std=[0.2673, 0.2564, 0.2762]), # CIFRAR100 Train stats\n",
    "    Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) # Use ImageNet stats\n",
    "])\n",
    "\n",
    "eval_transform = Compose([\n",
    "    Resize(256), # CIFRA100 is originally 32x32\n",
    "    CenterCrop(224), # But Dino works on 224x224\n",
    "    ToTensor(),\n",
    "    #Normalize(mean=[0.5071, 0.4866, 0.4409], std=[0.2673, 0.2564, 0.2762]), # CIFRAR100 Train stats\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Use ImageNet stats\n",
    "])\n",
    "\n",
    "cifar100_train = datasets.CIFAR100(root=RAW_DATA_DIR, train=True, download=True, transform=train_transform)\n",
    "cifar100_test = datasets.CIFAR100(root=RAW_DATA_DIR, train=False, download=True, transform=eval_transform)\n",
    "\n",
    "# Always specify a random state, otherwise the split will be different each time \n",
    "# If you have intention to load a model mid-training, you may have part of the validation set as already seen in the previous training\n",
    "train_dataset, val_dataset = train_test_split(cifar100_train, 0.8, random_state=42)\n",
    "test_dataset = cifar100_test\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Settings\n",
    "CHECKPOINT_DIR = \"/home/massimiliano/Projects/fl-g13/checkpoints\"\n",
    "name=\"arbok\"\n",
    "start_epoch=1\n",
    "num_epochs=80\n",
    "save_every=5\n",
    "backup_every=20\n",
    "\n",
    "# Model Hyper-parameters\n",
    "head_layers=3\n",
    "head_hidden_size=512\n",
    "dropout_rate=0.0\n",
    "unfreeze_blocks=1\n",
    "\n",
    "# Training Hyper-parameters\n",
    "batch_size=128\n",
    "lr=1e-3\n",
    "momentum=0.9\n",
    "weight_decay=1e-5\n",
    "T_max=8\n",
    "eta_min=1e-5\n",
    "\n",
    "# Dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model\n",
    "model = BaseDino(\n",
    "    head_layers=head_layers, \n",
    "    head_hidden_size=head_hidden_size, \n",
    "    dropout_rate=dropout_rate, \n",
    "    unfreeze_blocks=unfreeze_blocks\n",
    "    )\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer, scheduler, and loss function\n",
    "optimizer = SGD(\n",
    "    model.parameters(), \n",
    "    lr=lr,\n",
    "    momentum=momentum,\n",
    "    weight_decay=weight_decay\n",
    "    )\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer=optimizer, \n",
    "    T_max=T_max, \n",
    "    eta_min=eta_min\n",
    "    )\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "all_training_losses=[]       # Pre-allocated list for training losses\n",
    "all_validation_losses=[]       # Pre-allocated list for validation losses\n",
    "all_training_accuracies=[]    # Pre-allocated list for training accuracies\n",
    "all_validation_accuracies=[]    # Pre-allocated list for validation accuracies\n",
    "\n",
    "# ---- RESUME ----\n",
    "\n",
    "# # Model loading (uncomment to properly overwrite)\n",
    "# loading_epoch = 80\n",
    "# model, start_epoch = load(\n",
    "#     f\"{CHECKPOINT_DIR}/BaseDino/{name}_BaseDino_epoch_{loading_epoch}.pth\",\n",
    "#     model_class=BaseDino,\n",
    "#     device=device,\n",
    "#     optimizer=optimizer,\n",
    "#     scheduler=scheduler,\n",
    "#     verbose=True\n",
    "# )\n",
    "# model.to(device)\n",
    "# loaded_metrics = load_loss_and_accuracies(path=f\"{CHECKPOINT_DIR}/BaseDino/{name}_BaseDino_epoch_{loading_epoch}.loss_acc.json\")\n",
    "\n",
    "# # Preallocated lists: if the training interrupts, it will still save their values (uncomment to properly load and overwrite)\n",
    "# all_training_losses=loaded_metrics[\"train_loss\"]       # Pre-allocated list for training losses\n",
    "# all_validation_losses=loaded_metrics[\"val_loss\"]       # Pre-allocated list for validation losses\n",
    "# all_training_accuracies=loaded_metrics[\"train_acc\"]    # Pre-allocated list for training accuracies\n",
    "# all_validation_accuracies=loaded_metrics[\"val_acc\"]    # Pre-allocated list for validation accuracies\n",
    "\n",
    "# -----------------\n",
    "\n",
    "print(f\"\\nModel: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eecebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch of data from the test dataloader\n",
    "data_iter = iter(test_dataloader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Move the data to the same device as the model\n",
    "images = images.to(device)\n",
    "\n",
    "# Perform prediction\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Print the first prediction and its corresponding label\n",
    "print(f\"Predicted class: {predicted[0].item()}, True class: {labels[0].item()}\")\n",
    "print(f\"Outputs shape: {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feed087",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _, _, _, _ = train(\n",
    "        checkpoint_dir=CHECKPOINT_DIR,\n",
    "        name=name,\n",
    "        start_epoch=start_epoch,\n",
    "        num_epochs=num_epochs,\n",
    "        save_every=save_every,\n",
    "        backup_every=backup_every,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        verbose=1,\n",
    "        all_training_losses=all_training_losses,\n",
    "        all_validation_losses=all_validation_losses,\n",
    "        all_training_accuracies=all_training_accuracies,\n",
    "        all_validation_accuracies=all_validation_accuracies,\n",
    "    )\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted manually.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Training stopped due to error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f32900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(all_training_losses, label='Training Loss')\n",
    "plt.plot(all_validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)  # Add grid\n",
    "plt.xticks(range(0, len(all_training_losses)+1, 2), rotation=45)  # Skip every other tick\n",
    "\n",
    "# Plot accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(all_training_accuracies, label='Training Accuracy')\n",
    "plt.plot(all_validation_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)  # Add grid\n",
    "plt.xticks(range(0, len(all_training_accuracies)+1, 2), rotation=45)  # Skip every other tick\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation dataset\n",
    "test_loss, test_accuracy, _ = eval(dataloader=test_dataloader, model=model, criterion=criterion)\n",
    "\n",
    "print(\n",
    "    f\"🔍 Test Results:\\n\"\n",
    "    f\"\\t📉 Test Loss: {test_loss:.4f}\\n\"\n",
    "    f\"\\t🎯 Test Accuracy: {100 * test_accuracy:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f3a86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-g13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
