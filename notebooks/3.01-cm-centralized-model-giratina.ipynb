{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbee39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f52dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, RandomCrop, RandomHorizontalFlip, RandomVerticalFlip, Normalize, ToTensor\n",
    "\n",
    "from fl_g13.config import RAW_DATA_DIR\n",
    "from fl_g13.modeling import train, eval, save, load, backup\n",
    "from fl_g13.dataset import train_test_split\n",
    "\n",
    "from fl_g13.architectures import BaseDino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f11d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing pipeline\n",
    "train_transform = Compose([\n",
    "    Resize(256), # CIFRA100 is originally 32x32\n",
    "    RandomCrop(224), # But Dino works on 224x224\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5071, 0.4866, 0.4409], std=[0.2673, 0.2564, 0.2762]),\n",
    "])\n",
    "\n",
    "eval_transform = Compose([\n",
    "    Resize(256), # CIFRA100 is originally 32x32\n",
    "    CenterCrop(224), # But Dino works on 224x224\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5071, 0.4866, 0.4409], std=[0.2673, 0.2564, 0.2762]),\n",
    "])\n",
    "\n",
    "cifar100_train = datasets.CIFAR100(root=RAW_DATA_DIR, train=True, download=True, transform=train_transform)\n",
    "cifar100_test = datasets.CIFAR100(root=RAW_DATA_DIR, train=False, download=True, transform=eval_transform)\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(cifar100_train, 0.8, random_state=None)\n",
    "test_dataset = cifar100_test\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Settings\n",
    "CHECKPOINT_DIR = \"/home/massimiliano/Projects/fl-g13/checkpoints\"\n",
    "name = \"giratina\"\n",
    "start_epoch=1\n",
    "num_epochs=50\n",
    "save_every=5\n",
    "backup_every=10\n",
    "\n",
    "# Hyper-parameters\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-2\n",
    "\n",
    "# Dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model\n",
    "model = BaseDino(head_layers=3, head_hidden_size=1024, dropout_rate=0.15)\n",
    "model.to(device)\n",
    "# Optimizer, scheduler, and loss function\n",
    "optimizer = SGD(model.parameters(), lr=LR)\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer, \n",
    "    T_0=10,           # First restart after 10 epochs\n",
    "    T_mult=2,        # Double the interval between restarts each time\n",
    "    eta_min=1e-5     # Minimum learning rate after annealing\n",
    ")\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Model loading (uncomment to properly overwrite)\n",
    "# model, start_epoch = load(\n",
    "#     f\"{CHECKPOINT_DIR}/BaseDino/giratina_BaseDino_epoch_5.pth\",\n",
    "#     model_class=BaseDino,\n",
    "#     device=device,\n",
    "#     optimizer=optimizer,\n",
    "#     scheduler=scheduler,\n",
    "#     verbose=True\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "print(f\"\\nModel: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db93142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preallocated lists: if the training interrupts, it will still save their values\n",
    "all_training_losses=[]       # Pre-allocated list for training losses\n",
    "all_validation_losses=[]     # Pre-allocated list for validation losses\n",
    "all_training_accuracies=[]   # Pre-allocated list for training accuracies\n",
    "all_validation_accuracies=[] # Pre-allocated list for validation accuracies\n",
    "\n",
    "try:\n",
    "    _, _, _, _ = train(\n",
    "        checkpoint_dir=CHECKPOINT_DIR,\n",
    "        name=name,\n",
    "        start_epoch=start_epoch,\n",
    "        num_epochs=num_epochs,\n",
    "        save_every=save_every,\n",
    "        backup_every=backup_every,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        verbose=False,\n",
    "        all_training_losses=all_training_losses,\n",
    "        all_validation_losses=all_validation_losses,\n",
    "        all_training_accuracies=all_training_accuracies,\n",
    "        all_validation_accuracies=all_validation_accuracies,\n",
    "    )\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted manually. Backing up latest checkpoint...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Training stopped due to error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # This always runs no matter what (hopefully)\n",
    "    backup(f\"{CHECKPOINT_DIR}/{name}\") # Backup the final checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(all_training_losses, label='Training Loss')\n",
    "plt.plot(all_validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(all_training_accuracies, label='Training Accuracy')\n",
    "plt.plot(all_validation_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation dataset\n",
    "test_loss, test_accuracy, _ = eval(dataloader=test_dataloader, model=model, criterion=criterion)\n",
    "\n",
    "print(\n",
    "    f\"üîç Test Results:\\n\"\n",
    "    f\"\\tüìâ Test Loss: {test_loss:.4f}\\n\"\n",
    "    f\"\\tüéØ Test Accuracy: {100 * test_accuracy:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6b2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-g13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
