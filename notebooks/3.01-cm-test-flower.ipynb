{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d326f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f626291",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:24:24.400308Z",
     "start_time": "2025-05-03T07:24:14.251033Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-15 16:49:57.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfl_g13.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /home/massimiliano/Projects/fl-g13\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flower 1.17.0 / PyTorch 2.6.0+cu124\n",
      "'vision_transformer.py' already exists.\n",
      "'utils.py' already exists.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fl_g13.editing.sparseSGDM import SparseSGDM\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import flwr\n",
    "from flwr.simulation import run_simulation\n",
    "from fl_g13.architectures import BaseDino\n",
    "from fl_g13.fl_pytorch import get_client_app, get_server_app\n",
    "from fl_g13.fl_pytorch import build_fl_dependencies\n",
    "\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "\n",
    "build_fl_dependencies() #! Remind to always put this, it will download Dino dependencies for client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e9d3ca8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:24:24.415306Z",
     "start_time": "2025-05-03T07:24:24.401288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "CHECKPOINT_DIR = \"/home/massimiliano/Projects/fl-g13/checkpoints\"\n",
    "\n",
    "# Model hyper-parameters\n",
    "head_layers=3\n",
    "head_hidden_size=512\n",
    "dropout_rate=0.0\n",
    "unfreeze_blocks=1\n",
    "\n",
    "# Training hyper-parameters\n",
    "starting_lr = 1e-3\n",
    "momentum = 0.9\n",
    "weight_decay=1e-5\n",
    "T_max=8\n",
    "eta_min=1e-5\n",
    "\n",
    "# Federated Training setting\n",
    "batch_size = 64 # Batch size for training #! Let's stick to 64 to make training fit also on RTX 3070\n",
    "local_epochs = 2 # Number of local epochs per client\n",
    "number_of_rounds = 5 # Total number of federated learning rounds\n",
    "fraction_fit = 1 # Fraction of clients participating in training per round\n",
    "fraction_evaluate = 0.1 # Fraction of clients participating in evaluation per round\n",
    "number_of_clients = 3 # Total number of clients in the simulation\n",
    "min_num_clients = 2 # Minimum number of clients required for training and evaluation\n",
    "partition_type = \"iid\" # Partitioning strategy for the dataset (e.g., \"iid\" or \"shard\")\n",
    "num_shards_per_partition = 6 # Number of shards per partition (used when partition_type is \"shard\")\n",
    "use_wandb = False # Whether to use Weights & Biases (wandb) for experiment tracking (#!TODO, double check it works)\n",
    "\n",
    "# Device settings\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "backend_config = {\n",
    "    \"client_resources\": {\n",
    "        \"num_cpus\": 1, \n",
    "        \"num_gpus\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# When running on GPU, assign an entire GPU for each client\n",
    "# Refer to Flower framework documentation for more details about Flower simulations\n",
    "# and how to set up the `backend_config`\n",
    "if device == \"cuda\":\n",
    "    backend_config[\"client_resources\"] = {\"num_cpus\": 1, \"num_gpus\": 1}\n",
    "\n",
    "print(f\"Training on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aebfcdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:24:34.527814Z",
     "start_time": "2025-05-03T07:24:32.944367Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/massimiliano/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No checkpoint found at /home/massimiliano/Projects/fl-g13/checkpoints. Creating a new model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/massimiliano/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = BaseDino(\n",
    "    head_layers=head_layers, \n",
    "    head_hidden_size=head_hidden_size, \n",
    "    dropout_rate=dropout_rate, \n",
    "    unfreeze_blocks=unfreeze_blocks\n",
    "    )\n",
    "model.to(device)\n",
    "\n",
    "mask = [torch.ones_like(p, device=p.device) for p in model.parameters()] # Must be done AFTER the model is moved to CUDA\n",
    "optimizer = SparseSGDM(\n",
    "    model.parameters(),\n",
    "    mask=mask,\n",
    "    lr=starting_lr,\n",
    "    momentum=momentum,\n",
    "    weight_decay=weight_decay\n",
    "    )\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer=optimizer, \n",
    "    T_max=T_max, \n",
    "    eta_min=eta_min\n",
    "    )\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "client_app = get_client_app(\n",
    "    model=model, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,\n",
    "    device=device, \n",
    "    partition_type=partition_type, \n",
    "    batch_size=batch_size,\n",
    "    num_shards_per_partition=num_shards_per_partition,\n",
    "    local_epochs=local_epochs,\n",
    "    model_editing=False,\n",
    ")\n",
    "server_app = get_server_app(\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    prefix='aron', #! Introduced, you are force to pass this to avoid overwrites, if you pass an already used name it will load the most recent checkpoint\n",
    "    model_class=model.__class__,\n",
    "    model_config=model.get_config(), \n",
    "    optimizer=optimizer, \n",
    "    criterion=criterion, \n",
    "    scheduler=scheduler,\n",
    "    device=device, \n",
    "    save_every=1,\n",
    "    save_with_model_dir=False, #! Introduced: will save under {checkpoint path provided}/BaseDino/ dir if set to True\n",
    "    num_rounds=number_of_rounds, \n",
    "    fraction_fit=fraction_fit,\n",
    "    fraction_evaluate=fraction_evaluate,\n",
    "    min_fit_clients=min_num_clients,\n",
    "    min_evaluate_clients=min_num_clients,\n",
    "    min_available_clients=number_of_clients,\n",
    "    use_wandb=False,\n",
    "    wandb_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e94d4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-03T07:24:34.559663Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Server on device: cuda:0\n",
      "[Server] CUDA available in client: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=5, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using strategy 'CustomFedAvg' (default option)\n",
      "[Server Eval Round 0] Model device: cuda:0\n",
      "[Server Eval Round 0] CUDA available in server eval: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval progress: 100%|██████████| 313/313 [00:22<00:00, 13.69batch/s]\n",
      "\u001b[92mINFO \u001b[0m:      [Round 0] Centralized Evaluation - Loss: 6.5607, Metrics: {'centralized_accuracy': 0.0094}\n",
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 6.560712768627813, {'centralized_accuracy': 0.0094}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m 2025-05-15 16:50:24.786 | INFO     | fl_g13.config:<module>:11 - PROJ_ROOT path is: /home/massimiliano/Projects/fl-g13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m [Client] Client on device: cuda:0\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m [Client] CUDA available in client: True\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m No prefix/name for the model was provided, choosen prefix/name: quirky_pidgey_43\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m /home/massimiliano/miniconda3/envs/fl-g13/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:217: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m 🚀 Epoch 1/2 (50.00%) Completed\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t📊 Training Loss: 2.7556\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t✅ Training Accuracy: 34.44%\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t⏳ Elapsed Time: 36.72s | ETA: 36.72s\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t🕒 Completed At: 16:51\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m 🚀 Epoch 2/2 (100.00%) Completed\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t📊 Training Loss: 1.2625\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t✅ Training Accuracy: 63.92%\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t⏳ Elapsed Time: 37.46s | ETA: 0.00s\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t🕒 Completed At: 16:51\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m [Client] Client on device: cuda:0\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m [Client] CUDA available in client: True\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m No prefix/name for the model was provided, choosen prefix/name: funky_caterpie_29\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m 🚀 Epoch 1/2 (50.00%) Completed\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t📊 Training Loss: 2.6752\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t✅ Training Accuracy: 35.98%\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t⏳ Elapsed Time: 36.29s | ETA: 36.29s\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t🕒 Completed At: 16:52\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m 🚀 Epoch 2/2 (100.00%) Completed\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t📊 Training Loss: 1.2331\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t✅ Training Accuracy: 64.40%\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t⏳ Elapsed Time: 35.89s | ETA: 0.00s\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t🕒 Completed At: 16:52\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m [Client] Client on device: cuda:0\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m [Client] CUDA available in client: True\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m No prefix/name for the model was provided, choosen prefix/name: spunky_nidoran_16\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m 🚀 Epoch 1/2 (50.00%) Completed\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t📊 Training Loss: 2.6751\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t✅ Training Accuracy: 35.52%\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t⏳ Elapsed Time: 35.52s | ETA: 35.52s\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t🕒 Completed At: 16:53\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 3 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m 🚀 Epoch 2/2 (100.00%) Completed\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t📊 Training Loss: 1.2510\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t✅ Training Accuracy: 64.41%\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t⏳ Elapsed Time: 36.80s | ETA: 0.00s\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \t🕒 Completed At: 16:54\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      [Round 1] Avg Drift: 3.3525 | Relative Drift: 0.0065\n",
      "\u001b[92mINFO \u001b[0m:      [Round 1] Saving aggregated model at epoch 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint at: /home/massimiliano/Projects/fl-g13/checkpoints/fl_aron_BaseDino_epoch_1.pth\n",
      "[Server Eval Round 1] Model device: cuda:0\n",
      "[Server Eval Round 1] CUDA available in server eval: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval progress: 100%|██████████| 313/313 [00:22<00:00, 13.71batch/s]\n",
      "\u001b[92mINFO \u001b[0m:      [Round 1] Centralized Evaluation - Loss: 1.2075, Metrics: {'centralized_accuracy': 0.6586}\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (1, 1.2075117809323077, {'centralized_accuracy': 0.6586}, 252.56478277600036)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m [Client] Client on device: cuda:0\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m [Client] CUDA available in client: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval progress:   0%|          | 0/53 [00:00<?, ?batch/s]\n",
      "Eval progress:   2%|▏         | 1/53 [00:00<00:08,  6.01batch/s]\n",
      "Eval progress:   4%|▍         | 2/53 [00:00<00:08,  5.89batch/s]\n",
      "Eval progress:   6%|▌         | 3/53 [00:00<00:08,  5.98batch/s]\n",
      "Eval progress:   8%|▊         | 4/53 [00:00<00:08,  6.02batch/s]\n",
      "Eval progress:   9%|▉         | 5/53 [00:00<00:07,  6.05batch/s]\n",
      "Eval progress:  11%|█▏        | 6/53 [00:00<00:07,  6.05batch/s]\n",
      "Eval progress:  13%|█▎        | 7/53 [00:01<00:07,  6.07batch/s]\n",
      "Eval progress:  15%|█▌        | 8/53 [00:01<00:07,  6.10batch/s]\n",
      "Eval progress:  17%|█▋        | 9/53 [00:01<00:07,  6.12batch/s]\n",
      "Eval progress:  19%|█▉        | 10/53 [00:01<00:07,  6.10batch/s]\n",
      "Eval progress:  21%|██        | 11/53 [00:01<00:06,  6.09batch/s]\n",
      "Eval progress:  23%|██▎       | 12/53 [00:01<00:06,  6.13batch/s]\n",
      "Eval progress:  25%|██▍       | 13/53 [00:02<00:06,  6.13batch/s]\n",
      "Eval progress:  26%|██▋       | 14/53 [00:02<00:06,  6.13batch/s]\n",
      "Eval progress:  28%|██▊       | 15/53 [00:02<00:06,  6.13batch/s]\n",
      "Eval progress:  30%|███       | 16/53 [00:02<00:06,  6.11batch/s]\n",
      "Eval progress:  32%|███▏      | 17/53 [00:02<00:05,  6.09batch/s]\n",
      "Eval progress:  34%|███▍      | 18/53 [00:02<00:05,  6.11batch/s]\n",
      "Eval progress:  36%|███▌      | 19/53 [00:03<00:05,  6.10batch/s]\n",
      "Eval progress:  38%|███▊      | 20/53 [00:03<00:05,  6.11batch/s]\n",
      "Eval progress:  40%|███▉      | 21/53 [00:03<00:05,  6.10batch/s]\n",
      "Eval progress:  42%|████▏     | 22/53 [00:03<00:05,  6.09batch/s]\n",
      "Eval progress:  43%|████▎     | 23/53 [00:03<00:04,  6.10batch/s]\n",
      "Eval progress:  45%|████▌     | 24/53 [00:03<00:04,  6.10batch/s]\n",
      "Eval progress:  47%|████▋     | 25/53 [00:04<00:04,  6.12batch/s]\n",
      "Eval progress:  49%|████▉     | 26/53 [00:04<00:04,  6.14batch/s]\n",
      "Eval progress:  51%|█████     | 27/53 [00:04<00:04,  6.16batch/s]\n",
      "Eval progress:  53%|█████▎    | 28/53 [00:04<00:04,  6.15batch/s]\n",
      "Eval progress:  55%|█████▍    | 29/53 [00:04<00:03,  6.12batch/s]\n",
      "Eval progress:  57%|█████▋    | 30/53 [00:04<00:03,  6.15batch/s]\n",
      "Eval progress:  58%|█████▊    | 31/53 [00:05<00:03,  6.15batch/s]\n",
      "Eval progress:  60%|██████    | 32/53 [00:05<00:03,  6.13batch/s]\n",
      "Eval progress:  62%|██████▏   | 33/53 [00:05<00:03,  6.15batch/s]\n",
      "Eval progress:  64%|██████▍   | 34/53 [00:05<00:03,  6.15batch/s]\n",
      "Eval progress:  66%|██████▌   | 35/53 [00:05<00:02,  6.16batch/s]\n",
      "Eval progress:  68%|██████▊   | 36/53 [00:05<00:02,  6.15batch/s]\n",
      "Eval progress:  70%|██████▉   | 37/53 [00:06<00:02,  6.15batch/s]\n",
      "Eval progress:  72%|███████▏  | 38/53 [00:06<00:02,  6.16batch/s]\n",
      "Eval progress:  74%|███████▎  | 39/53 [00:06<00:02,  6.14batch/s]\n",
      "Eval progress:  75%|███████▌  | 40/53 [00:06<00:02,  6.10batch/s]\n",
      "Eval progress:  77%|███████▋  | 41/53 [00:06<00:01,  6.11batch/s]\n",
      "Eval progress:  79%|███████▉  | 42/53 [00:06<00:01,  6.10batch/s]\n",
      "Eval progress:  81%|████████  | 43/53 [00:07<00:01,  6.12batch/s]\n",
      "Eval progress:  83%|████████▎ | 44/53 [00:07<00:01,  6.15batch/s]\n",
      "Eval progress:  85%|████████▍ | 45/53 [00:07<00:01,  6.17batch/s]\n",
      "Eval progress:  87%|████████▋ | 46/53 [00:07<00:01,  6.16batch/s]\n",
      "Eval progress:  89%|████████▊ | 47/53 [00:07<00:00,  6.16batch/s]\n",
      "Eval progress:  91%|█████████ | 48/53 [00:07<00:00,  6.17batch/s]\n",
      "Eval progress:  92%|█████████▏| 49/53 [00:08<00:00,  6.17batch/s]\n",
      "Eval progress:  94%|█████████▍| 50/53 [00:08<00:00,  6.17batch/s]\n",
      "Eval progress:  96%|█████████▌| 51/53 [00:08<00:00,  6.16batch/s]\n",
      "Eval progress: 100%|██████████| 53/53 [00:08<00:00,  6.23batch/s]\n",
      "Eval progress:   0%|          | 0/53 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m [Client] Client on device: cuda:0\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m [Client] CUDA available in client: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval progress:   2%|▏         | 1/53 [00:00<00:08,  6.10batch/s]\n",
      "Eval progress:   4%|▍         | 2/53 [00:00<00:08,  6.15batch/s]\n",
      "Eval progress:   6%|▌         | 3/53 [00:00<00:08,  6.07batch/s]\n",
      "Eval progress:   8%|▊         | 4/53 [00:00<00:08,  6.12batch/s]\n",
      "Eval progress:   9%|▉         | 5/53 [00:00<00:07,  6.15batch/s]\n",
      "Eval progress:  11%|█▏        | 6/53 [00:00<00:07,  6.18batch/s]\n",
      "Eval progress:  13%|█▎        | 7/53 [00:01<00:07,  6.16batch/s]\n",
      "Eval progress:  15%|█▌        | 8/53 [00:01<00:07,  6.10batch/s]\n",
      "Eval progress:  17%|█▋        | 9/53 [00:01<00:07,  6.11batch/s]\n",
      "Eval progress:  19%|█▉        | 10/53 [00:01<00:07,  6.09batch/s]\n",
      "Eval progress:  21%|██        | 11/53 [00:01<00:06,  6.06batch/s]\n",
      "Eval progress:  23%|██▎       | 12/53 [00:01<00:06,  6.08batch/s]\n",
      "Eval progress:  25%|██▍       | 13/53 [00:02<00:06,  6.08batch/s]\n",
      "Eval progress:  26%|██▋       | 14/53 [00:02<00:06,  5.96batch/s]\n",
      "Eval progress:  28%|██▊       | 15/53 [00:02<00:06,  5.93batch/s]\n",
      "Eval progress:  30%|███       | 16/53 [00:02<00:06,  5.96batch/s]\n",
      "Eval progress:  32%|███▏      | 17/53 [00:02<00:06,  5.97batch/s]\n",
      "Eval progress:  34%|███▍      | 18/53 [00:02<00:05,  5.97batch/s]\n",
      "Eval progress:  36%|███▌      | 19/53 [00:03<00:05,  5.96batch/s]\n",
      "Eval progress:  38%|███▊      | 20/53 [00:03<00:05,  5.96batch/s]\n",
      "Eval progress:  40%|███▉      | 21/53 [00:03<00:05,  5.98batch/s]\n",
      "Eval progress:  42%|████▏     | 22/53 [00:03<00:05,  5.99batch/s]\n",
      "Eval progress:  43%|████▎     | 23/53 [00:03<00:05,  5.98batch/s]\n",
      "Eval progress:  45%|████▌     | 24/53 [00:03<00:04,  5.98batch/s]\n",
      "Eval progress:  47%|████▋     | 25/53 [00:04<00:04,  5.97batch/s]\n",
      "Eval progress:  49%|████▉     | 26/53 [00:04<00:04,  5.97batch/s]\n",
      "Eval progress:  51%|█████     | 27/53 [00:04<00:04,  5.99batch/s]\n",
      "Eval progress:  53%|█████▎    | 28/53 [00:04<00:04,  6.00batch/s]\n",
      "Eval progress:  55%|█████▍    | 29/53 [00:04<00:03,  6.00batch/s]\n",
      "Eval progress:  57%|█████▋    | 30/53 [00:04<00:03,  6.01batch/s]\n",
      "Eval progress:  58%|█████▊    | 31/53 [00:05<00:03,  6.00batch/s]\n",
      "Eval progress:  60%|██████    | 32/53 [00:05<00:03,  5.99batch/s]\n",
      "Eval progress:  62%|██████▏   | 33/53 [00:05<00:03,  5.98batch/s]\n",
      "Eval progress:  64%|██████▍   | 34/53 [00:05<00:03,  5.97batch/s]\n",
      "Eval progress:  66%|██████▌   | 35/53 [00:05<00:03,  5.95batch/s]\n",
      "Eval progress:  68%|██████▊   | 36/53 [00:05<00:02,  5.97batch/s]\n",
      "Eval progress:  70%|██████▉   | 37/53 [00:06<00:02,  5.97batch/s]\n",
      "Eval progress:  72%|███████▏  | 38/53 [00:06<00:02,  5.99batch/s]\n",
      "Eval progress:  74%|███████▎  | 39/53 [00:06<00:02,  6.01batch/s]\n",
      "Eval progress:  75%|███████▌  | 40/53 [00:06<00:02,  5.99batch/s]\n",
      "Eval progress:  77%|███████▋  | 41/53 [00:06<00:02,  5.99batch/s]\n",
      "Eval progress:  79%|███████▉  | 42/53 [00:06<00:01,  5.99batch/s]\n",
      "Eval progress:  81%|████████  | 43/53 [00:07<00:01,  5.99batch/s]\n",
      "Eval progress:  83%|████████▎ | 44/53 [00:07<00:01,  5.99batch/s]\n",
      "Eval progress:  85%|████████▍ | 45/53 [00:07<00:01,  5.93batch/s]\n",
      "Eval progress:  87%|████████▋ | 46/53 [00:07<00:01,  5.98batch/s]\n",
      "Eval progress:  89%|████████▊ | 47/53 [00:07<00:01,  5.97batch/s]\n",
      "Eval progress:  91%|█████████ | 48/53 [00:07<00:00,  5.99batch/s]\n",
      "Eval progress:  92%|█████████▏| 49/53 [00:08<00:00,  5.99batch/s]\n",
      "Eval progress:  94%|█████████▍| 50/53 [00:08<00:00,  6.02batch/s]\n",
      "Eval progress:  96%|█████████▌| 51/53 [00:08<00:00,  6.03batch/s]\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n",
      "Eval progress: 100%|██████████| 53/53 [00:08<00:00,  6.11batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m [Client] Client on device: cuda:0\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m [Client] CUDA available in client: True\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m No prefix/name for the model was provided, choosen prefix/name: silly_nidoran_21\n",
      "\u001b[36m(ClientAppActor pid=86603)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "run_simulation(\n",
    "    client_app=client_app,\n",
    "    server_app=server_app,\n",
    "    num_supernodes=number_of_clients,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c258a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-g13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
