{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed3f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecab0c6",
   "metadata": {},
   "source": [
    "# Import cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr\n",
    "import torch\n",
    "import dotenv\n",
    "import wandb\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from fl_g13.architectures import BaseDino\n",
    "from fl_g13.editing import SparseSGDM\n",
    "\n",
    "from fl_g13.fl_pytorch import build_fl_dependencies\n",
    "\n",
    "from fl_g13.fl_pytorch.datasets import reset_partition\n",
    "from fl_g13.modeling import load_or_create\n",
    "from fl_g13.editing import SparseSGDM\n",
    "from torch.optim import SGD\n",
    "\n",
    "from fl_g13.fl_pytorch import get_client_app, get_server_app\n",
    "from flwr.simulation import run_simulation\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a28c0",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "build_fl_dependencies()\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0}}\n",
    "else:\n",
    "    backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4be89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login by key in .env file\n",
    "WANDB_API_KEY = dotenv.dotenv_values()[\"WANDB_API_KEY\"]\n",
    "wandb.login(key=WANDB_API_KEY)\n",
    "\n",
    "# Load checkpoint from .env file\n",
    "CHECKPOINT_DIR = dotenv.dotenv_values()[\"CHECKPOINT_DIR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7eefab",
   "metadata": {},
   "source": [
    "# Training parameters definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9668493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FL config\n",
    "K = 100\n",
    "C = 0.1\n",
    "Js = [4, 8, 16]\n",
    "num_shards_per_partition = [1, 5, 10, 50] # Nc\n",
    "partition_type = 'shard'\n",
    "\n",
    "num_rounds = 200\n",
    "\n",
    "## Server App config\n",
    "save_every = 5\n",
    "evaluate_each = 5\n",
    "fraction_fit = C        # 0.1\n",
    "fraction_evaluate = C   # 0.1\n",
    "min_fit_clients = 10\n",
    "min_evaluate_clients = 5\n",
    "min_available_clients = 10\n",
    "\n",
    "# model editing config\n",
    "model_editing = False\n",
    "mask = None\n",
    "\n",
    "## simulation run config\n",
    "NUM_CLIENTS = 100\n",
    "MAX_PARALLEL_CLIENTS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd06cef",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d96a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "## Model Hyper-parameters\n",
    "head_layers = 3\n",
    "head_hidden_size = 512\n",
    "dropout_rate = 0.0\n",
    "unfreeze_blocks = 0\n",
    "\n",
    "## Training Hyper-parameters\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-5\n",
    "\n",
    "def load_model_for_simulation(shard, J):\n",
    "    ## Base model location\n",
    "    model_save_path = CHECKPOINT_DIR + f\"/fl/non-iid/warmup/{shard}_{J}\"\n",
    "    \n",
    "    model, start_epoch = load_or_create(\n",
    "        path=model_save_path,\n",
    "        model_class=BaseDino,\n",
    "        model_config=None,\n",
    "        optimizer=None,\n",
    "        scheduler=None,\n",
    "        device=DEVICE,\n",
    "        verbose=True,\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    if model_editing:\n",
    "        # Create a dummy mask for SparseSGDM\n",
    "        dummy_mask = [torch.ones_like(p, device=p.device) for p in model.parameters()]  \n",
    "        \n",
    "        optimizer = SparseSGDM(\n",
    "            model.parameters(),\n",
    "            mask=dummy_mask,\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "    else:\n",
    "        optimizer = SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "        \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = None\n",
    "    \n",
    "    return model, start_epoch, optimizer, criterion, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8776b0ec",
   "metadata": {},
   "source": [
    "# Run Flower Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d9cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all the simulations, one after the other\n",
    "for shards, J in product(num_shards_per_partition, Js):\n",
    "    print(f'||| Warm-up for {shards}_{J} |||\\n')\n",
    "    \n",
    "    reset_partition()\n",
    "    \n",
    "    model, start_epoch, optimizer, criterion, scheduler = load_model_for_simulation(shards, J)\n",
    "    model_checkpoint = CHECKPOINT_DIR + f\"/fl/non-iid/warmup/{shards}_{J}\"\n",
    "\n",
    "    # Wandb settings\n",
    "    use_wandb = False\n",
    "    run_name = f\"fl_{shards}_{J}\"\n",
    "    wandb_config = {\n",
    "        # wandb param\n",
    "        'name': run_name,\n",
    "        'project_name': f\"fl_v5_{shards}_{J}_warmup\",\n",
    "        'run_id': run_name,\n",
    "        \n",
    "        # fl config\n",
    "        \"fraction_fit\": fraction_fit,\n",
    "        \"lr\": lr,\n",
    "        \"momentum\": momentum,\n",
    "        'weight_decay': weight_decay,\n",
    "        'partition_type': partition_type,\n",
    "        'K': K,\n",
    "        'C': C,\n",
    "        'J': J,\n",
    "        'Nc': shards,\n",
    "        \n",
    "        # model config\n",
    "        'head_layers': head_layers,\n",
    "        'head_hidden_size': head_hidden_size,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'unfreeze_blocks': unfreeze_blocks,\n",
    "    }\n",
    "\n",
    "    client = get_client_app(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=DEVICE,\n",
    "        partition_type=partition_type,\n",
    "        local_epochs=1,\n",
    "        local_steps=J,\n",
    "        batch_size=batch_size,\n",
    "        num_shards_per_partition=shards,\n",
    "        scheduler=scheduler,\n",
    "        model_editing=model_editing,\n",
    "        mask=mask,\n",
    "        mask_func=None\n",
    "    )\n",
    "\n",
    "    server = get_server_app(\n",
    "        checkpoint_dir=model_checkpoint,\n",
    "        model_class=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        scheduler=scheduler,\n",
    "        num_rounds=num_rounds,\n",
    "        fraction_fit=fraction_fit,\n",
    "        fraction_evaluate=fraction_evaluate,\n",
    "        min_fit_clients=min_fit_clients,\n",
    "        min_evaluate_clients=min_evaluate_clients,\n",
    "        min_available_clients=min_available_clients,\n",
    "        device=DEVICE,\n",
    "        use_wandb=use_wandb,\n",
    "        wandb_config=wandb_config,\n",
    "        save_every=save_every,\n",
    "        prefix='warmup',\n",
    "        evaluate_each=evaluate_each,\n",
    "        model= model,\n",
    "        start_epoch= start_epoch\n",
    "    )\n",
    "\n",
    "    # Run simulation\n",
    "    run_simulation(\n",
    "        server_app=server,\n",
    "        client_app=client,\n",
    "        num_supernodes=NUM_CLIENTS,\n",
    "        backend_config=backend_config\n",
    "    )\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
