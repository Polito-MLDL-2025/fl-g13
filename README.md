# AdaQuo

<a target="_blank" href="https://cookiecutter-data-science.drivendata.org/">
    <img src="https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter" />
</a>

Federated Learning Project, Group 13

Nguyen Thanh, Gamba Stefano, Carli Massimiliano, Tarantino Giovanbattista

_We propose AdaQuo, a novel approach for sparse fine-tuning in Federated Learning designed to mitigate poor performances of highly non-IID setting. AdaQuo constructs a global sparsity mask by aggregating client-side binary masks. These masks are summed by the server as a form of voting, and a global mask is generated by selecting parameters that receive the highest number of votes. As training progresses, the server dynamically increases the vote threshold---based on observed average client drift---thereby adapting the level of sparsity over time. Experiments on CIFAR-100 demonstrate that AdaQuo outperforms decentralized sparse fine-tuned baselines in heterogeneous environments._

> [!NOTE]
> We use Flower for our federated learning implementation, you can find the original flower documentation [here](https://flower.ai/), and a summary of the main Flower tools we used [here](FLOWER.md).

## How to Install

### Using `make`

1. Ensure you have `make` installed on your system. If not, you can find installation guides for your operating system online (e.g., for Linux: `sudo apt install build-essential`, for macOS: `brew install make`, for Windows: `choco install make`).
2. Run the following command to set up the project environment:

    ```bash
    make install
    ```

    This will:
    - Create a new virtual enviroment (`venv`)
    - Install the required Python dependencies listed in `requirements.txt`.
    - Set up any additional configurations needed for the project.

### Using `venv`

1. Ensure you have Python installed on your system. This project was developed using Python 3.10, which is the suggested version.
2. Create a virtual environment and install the required dependencies:

    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    pip install -r requirements.txt
    ```

    This will:
    - Create a new virtual environment named `venv`.
    - Install the required Python dependencies listed in `requirements.txt`.
    - Set up the environment for the project.

### Using `conda`

1. Ensure you have `conda` installed on your system.
2. Create a new conda environment and install the required dependencies:

    ```bash
    conda create --name fl-g13 python=3.10
    conda activate fl-g13
    pip install -r requirements.txt
    ```

    This will:
    - Create a new conda environment named `fl-g13` with Python 3.10.
    - Install the required Python dependencies listed in `requirements.txt`.
    - Set up the environment for the project.

## Project Organization

All Flower simulations are run exclusively through the Jupyter notebooks located in the `notebooks/` directory. The Python source code modules can be found within `fl_g13/`. For the federated learning implementation please refer to `fl_g13/fl_pytorch` and read `FLOWER.md` to understand how Flower works 

```text
â”œâ”€â”€ LICENSE            <- Open-source license if one is chosen
â”œâ”€â”€ Makefile           <- Makefile with convenience commands for environment setup and notebook export
â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
â”œâ”€â”€ FLOWER.md          <- A guide describing the concepts of Flower necessary to understand our project.
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ external       <- Data from third party sources.
â”‚   â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
â”‚   â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
â”‚   â””â”€â”€ raw            <- The original, immutable data dump.
â”‚
â”œâ”€â”€ models             <- Checkpoints produced during training
â”‚
â”œâ”€â”€ notebooks          <- Jupyter notebooks.
â”‚
â”œâ”€â”€ pyproject.toml     <- Project configuration file with package metadata for 
â”‚                         fl_g13
â”‚
â”œâ”€â”€ requirements.txt   <- The requirements packages for reproducing experiments
â”‚
â””â”€â”€ fl_g13   <- Source code of this project.
    â”‚
    â”œâ”€â”€ __init__.py             <- Makes fl_g13 a Python module
    â”œâ”€â”€ config.py               <- Store useful variables and configuration
    â”œâ”€â”€ dataset.py              <- Scripts to download and manage data
    â”œâ”€â”€ architectures/          <- Classes for models architectures
    â”‚   â””â”€â”€ BaseDino.py
    â”œâ”€â”€ editing/                <- Code for model editing
    â”‚   â”œâ”€â”€ fisher.py
    â”‚   â”œâ”€â”€ masking.py
    â”‚   â””â”€â”€ sparseSGDM.py
    â”œâ”€â”€ fl_pytorch/             <- Code for Flower federated learning apps
    â”‚   â”œâ”€â”€ client_app.py
    â”‚   â”œâ”€â”€ client.py
    â”‚   â”œâ”€â”€ datasets.py
    â”‚   â”œâ”€â”€ DynamicQuorumClient.py
    â”‚   â”œâ”€â”€ DynamicQuorumStrategy.py
    â”‚   â”œâ”€â”€ model.py
    â”‚   â”œâ”€â”€ server_app.py
    â”‚   â”œâ”€â”€ strategy.py
    â”‚   â”œâ”€â”€ task.py
    â”‚   â”œâ”€â”€ utils.py
    â”‚   â””â”€â”€ editing/            <- Code for federated mask computation
    â”‚       â””â”€â”€ centralized_mask.py
    â””â”€â”€ modeling/               <- Code for train, test, save and load models
        â”œâ”€â”€ eval.py
        â”œâ”€â”€ load.py
        â”œâ”€â”€ train.py
        â””â”€â”€ utils.py
```

---

## How to contribute

### **Commit Message and Branch Naming Rules**

1. **Commit Message Format**
    - Use the following format for commit messages:
      ```
      <type>: <short description>
      ```
    - **Types**:
      - `feat`: A new feature
      - `fix`: A bug fix
      - `docs`: Documentation changes
      - `style`: Code style changes (formatting, missing semicolons, etc.)
      - `refactor`: Code refactoring without adding features or fixing bugs
    - Example:
      ```
      feat: add data preprocessing pipeline
      fix: resolve issue with model training script
      ```

2. **Branch Naming Convention**
    - Use the following format for branch names:
      ```
      <type>-<short-description>-<initials>
      ```
    - **Types**:
      - `feat`: For new features
      - `fix`: For bug fixes
      - `docs`: For documentation updates
      - `refactor`: For refactoring tasks
    - Example:
      ```
      feat-add-preprocessing-pipeline-pjb
      fix-model-training-bug-mc
      ```

### **Jupyter Notebook Usage**

1. **Notebook Organization**
    - Notebooks must be stored in the `notebooks/` directory.
    - Naming convention: `PHASE.NOTEBOOK-INITIALS-DESCRIPTION.ipynb`
        
        Example: `0.01-pjb-data-source-1.ipynb`
        
        - `PHASE` codes:
            - `0` â€“ Data exploration
            - `1` â€“ Data cleaning & feature engineering
            - `2` â€“ Visualization
            - `3` â€“ Modeling
            - `4` â€“ Publication
        - `INITIALS` â€“ Your initials; helps identify the author and avoid conflicts.
        - `DESCRIPTION` â€“ Short, clear description of the notebook's purpose.

### **Code Reusability & Refactoring Regulation**

1. **Refactor Shared Code into Modules**
    - Store reusable code in the `fl_g13` package.
    - Add the following cell at the top of each notebook:

    ```python
    %load_ext autoreload
    %autoreload 2
    ```

### **Code Review & Version Control Regulation**

1. **Use `nbautoexport` Tool**
    - Install with:

    ```bash
    nbautoexport install
    nbautoexport configure notebooks
    ```

    - Then, anytime you want to export a notebook to a Python script, run:

    ```bash
    nbautoexport export notebooks/<notebook_name>.ipynb
    ```

    - Equivalently, you can also run:

    ```bash
    make export
    ```

    for convenience â€” this will export **all** notebooks in the `notebooks/` folder automatically.

    - ðŸ’¡ **Pro Tip:** Add the following line at the end of each notebook to automatically export it every time you run it:

    ```python
    !nbautoexport export notebooks/<notebook_name>.ipynb
    ```

    or just do

    ```python
    !make export
    ```

2. **Ensure Reviewability**
     - Commit both `.ipynb` files and their exported `.py` versions to version control.

#### (PyCharm only) Use a Git Hook or File Watcher

You can set up PyCharm to run `nbconvert` (or even `nbautoexport export`) every time a file is saved or committed.

 **PyCharm File Watcher**

1. Go to **Settings > Tools > File Watchers**
2. Add a new watcher with the following configuration:

- **File Type**: Jupyter Notebook (`*.ipynb`)
- **Scope**: Current project
- **Program**: Your Python interpreter path (e.g., `python`)
- **Arguments**:

  ```bash
  -m nbautoexport export $FileDir$
  ```
- **Working Directory**: `$FileDir$`
