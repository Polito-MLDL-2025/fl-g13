### Reminders

- DO NOT FORGET TO TRAIN ON THE WHOLE DATASET BEFORE REALISING!

### Questions

13/05 Questions for Riccardo and Iurada
- Notes on paper

06/05 Questions for Riccardo
- Does it make sense to do a dropout also on frozen layers?
- Is it ok to overfitt the training if the validation is still increasing?
- Should we use test in the valudation phase? Or rather split a validation set from the train?
- Can we apply transformations on validation set?
- Will we be judged on the performances of the centralized baseline model? 
	Or we should rather focus on having decent accuracy with a small enough model and then move on?
	Just asking because some people may struggle to do many epochs or try different set of hyperparameters since Colab limitations
- Limit the number of parameters?\
- In centralized baseline does it make sense to make mask at class level?
- Mask is calculated at Client or Server level? Aggregation of client masks at server level?
- What about re-calibrating mask every n epochs?

15/04 Questions for Riccardo
- At "make centralized model" we need to make just a model as in labs, or rather pre-traing with some batches the server's model, which will then be shared with the rest of the network?
- Do we need to transform and augment our data? Then we tune using different optimizers. Can we use Adam?
- What is Dino? What are scores in the table? Can we use DinoV2?

### Guidelines on final model
- Few trainable parameters
- Use same normalization as ImageNet
- Use same transformations as Dino
- No fancy params: weight decay, momentum, warm restart, ...
