### Tuesday

- State of centralized baseline
- Explain SparseSGDM
- Show task vector
- Clean old models, decide the best -> another training?
- Discuss what is missing in the FedLearning baseline
- Discuss how to implement Model Editing in FedLearning

### Questions

06/05 Questions for Riccardo
- Does it make sense to do a dropout also on frozen layers?
- Is it ok to overfitt the training if the validation is still increasing?
- Should we use test in the valudation phase? Or rather split a validation set from the train?
- Can we apply transformations on validation set?
- Will we be judged on the performances of the centralized baseline model? 
	Or we should rather focus on having decent accuracy with a small enough model and then move on?
	Just asking because some people may struggle to do many epochs or try different set of hyperparameters since Colab limitations
- Limit the number of parameters?
- Mask is calculated at Client or Server level?

15/04 Questions for Riccardo
- At "make centralized model" we need to make just a model as in labs, or rather pre-traing with some batches the server's model, which will then be shared with the rest of the network?
- Do we need to transform and augment our data? Then we tune using different optimizers. Can we use Adam?
- What is Dino? What are scores in the table? Can we use DinoV2?

### Guidelines on final model
- Few trainable parameters
- Use same normalization as ImageNet
- Use same transformations as Dino
- No fancy params: weight decay, momentum, warm restart, ...

### Notes
- Cross validation
	X Try different learning_rates -> done by the scheduler
	X Try different batch_size -> 64, 128, 256
	X Try different layer_size -> 1024, 2048
	X Try different number_of_layers -> 3, 5, 8
	- Try CosineAnihilatorLR with right number of T steps (can you rather pass eta_min?)
	- Choose number of epochs
X Transformations to use:
	X Normalization
	X Random Crop
	X Horizontal Flip (no vert!) 
	X (Only normalization for Test)
X Optimizer? SGD
X Scheduler? CosineAnihilatorWarmRestart
X Dropout? No
X Drop paths? No
X momentum? weight_decay? nesterov? No
