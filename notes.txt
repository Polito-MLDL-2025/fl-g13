### Reminders

- DO NOT FORGET TO TRAIN ON THE WHOLE DATASET BEFORE REALISING!

### Questions

06/05 Questions for Riccardo
- Does it make sense to do a dropout also on frozen layers?
- Is it ok to overfitt the training if the validation is still increasing?
- Should we use test in the valudation phase? Or rather split a validation set from the train?
- Can we apply transformations on validation set?
- Will we be judged on the performances of the centralized baseline model? 
	Or we should rather focus on having decent accuracy with a small enough model and then move on?
	Just asking because some people may struggle to do many epochs or try different set of hyperparameters since Colab limitations
- Limit the number of parameters?\
- In centralized baseline does it make sense to make mask at class level?
- Mask is calculated at Client or Server level? Aggregation of client masks at server level?
- What about re-calibrating mask every n epochs?

- Q, K, bias of K will probably be less sensitive parameters;
- Probably sparsity to 90% onwards (10% of ones, 90% of zeros)
- Train the mask with batch_size = 1 (online training), and multiple rounds (i.e., epochs)
- See torchs's vmap routine, backpack (not implemented for all modules of Dino), torch.autograd.grad()
- Use backward(retain_grad=True)
- Optimizer SparseSGDM for not multiplying each param with 1s and 0s, but seting requires.grad() = False to appropriate elements

15/04 Questions for Riccardo
- At "make centralized model" we need to make just a model as in labs, or rather pre-traing with some batches the server's model, which will then be shared with the rest of the network?
- Do we need to transform and augment our data? Then we tune using different optimizers. Can we use Adam?
- What is Dino? What are scores in the table? Can we use DinoV2?

### Guidelines on final model
- Few trainable parameters
- Use same normalization as ImageNet
- Use same transformations as Dino
- No fancy params: weight decay, momentum, warm restart, ...

### Notes
- Cross validation
	X Try different learning_rates -> done by the scheduler
	X Try different batch_size -> 64, 128, 256
	X Try different layer_size -> 1024, 2048
	X Try different number_of_layers -> 3, 5, 8
	- Try CosineAnihilatorLR with right number of T steps (can you rather pass eta_min?)
	- Choose number of epochs
X Transformations to use:
	X Normalization
	X Random Crop
	X Horizontal Flip (no vert!) 
	X (Only normalization for Test)
X Optimizer? SGD
X Scheduler? CosineAnihilatorWarmRestart
X Dropout? No
X Drop paths? No
X momentum? weight_decay? nesterov? No
