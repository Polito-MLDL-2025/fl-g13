### Reminders

- DO NOT FORGET TO TRAIN ON THE WHOLE DATASET BEFORE REALISING!

### Questions

13/05 Questions for Riccardo and Iurada
- Warmup the model before doing sparse fine tuning, or experiment yourself
- Should do few-shots mask calibration

06/05 Questions for Riccardo and Iurada
- Does it make sense to do a dropout also on frozen layers?
- Is it ok to overfitt the training if the validation is still increasing?
- Should we use test in the valudation phase? Or rather split a validation set from the train?
- Can we apply transformations on validation set?
- Will we be judged on the performances of the centralized baseline model? 
	Or we should rather focus on having decent accuracy with a small enough model and then move on?
	Just asking because some people may struggle to do many epochs or try different set of hyperparameters since Colab limitations
- Limit the number of parameters?\
- In centralized baseline does it make sense to make mask at class level?
- Mask is calculated at Client or Server level? Aggregation of client masks at server level?
- What about re-calibrating mask every n epochs?

15/04 Questions for Riccardo
- At "make centralized model" we need to make just a model as in labs, or rather pre-traing with some batches the server's model, which will then be shared with the rest of the network?
- Do we need to transform and augment our data? Then we tune using different optimizers. Can we use Adam?
- What is Dino? What are scores in the table? Can we use DinoV2?

### Guidelines on final model
- Few trainable parameters
- Use same normalization as ImageNet
- Use same transformations as Dino
- No fancy params: weight decay, momentum, warm restart, ...

### What to ask to TA on private call
- In FL, should we start by a checkpoint? Or we start completely from scratch? Stefano approach vs Thanh approach
- What improvemente do you expect in model editing
- Too many hyper-parameters
- Proposals here below

--- [ Standard Proposals ] ---

1. Train clients with its mask local → send params (already masked) to server → server compute FedAvg as normal
2. Client send its fisher scores to server → server aggregate fisher score and compute the global mask → send to all client for training
3. Clients send masks to server → server aggregate mask and send back to all clients to keep training. Potential aggregate mask methods:
	Union
	Intersection
	Most common params ( more than k % clients want to update)
	Personalized mask for each client, based on their drift (lin combination of masks, in which drifts are the weights of such linear combination)
4. Arbitrary freeze part of the backbone and compare results

--- [ Hardcore Proposals ] ---

1. Learnable masks (meta-learning)

--- [ Other approaches, not really on ME ] ---

1. Completely decentralized FL --> P2P
2. Scores and masks compressions and quantizations
