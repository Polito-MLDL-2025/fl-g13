### Questions

29/04 Questions for Riccardo
- Does it make sense to do a dropout also on frozen layers?
- Can we apply transformations on validation set?
- Is it ok to overfitt the training if the validation is still increasing?
- Limit in number of parameters? 
- Should we use test in the valudation phase? Or rather split a validation set from the train?

15/04 Questions for Riccardo
- At "make centralized model" we need to make just a model as in labs, or rather pre-traing with some batches the server's model, which will then be shared with the rest of the network?
- Do we need to transform and augment our data? Then we tune using different optimizers. Can we use Adam?
- What is Dino? What are scores in the table? Can we use DinoV2?
- not seeing comments

### Notes
-Cross validation
	X Try different learning_rates -> done by the scheduler
	- Try different batch_size -> 64, 128, 256
	- Try different layer_size -> 1024, 2048
	- Try different number_of_layers -> 3, 5, 8
X Transformations to use:
	X Normalization
	X Random Crop
	X Horizontal Flip (no vert!) 
	X (Only normalization for Test)
X Optimizer? SGD
X Scheduler? CosineAnihilatorWarmRestart
X Dropout? Yes
X Drop paths? No
X momentum? weight_decay? nesterov? No
